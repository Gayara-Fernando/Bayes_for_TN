{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739470f-da32-43f3-a605-43daa8e7f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "from scipy import ndimage\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eab466-83c3-4127-a9d6-1ce77efb9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if the noetbook accesses the GPU\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba6141-5512-4ec7-84c5-abb67b358920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data locations\n",
    "block_0101 = '../../S_lab_TasselNet/Block_1_TN/Block_1_images_and_xml'\n",
    "block_0102 = '../../S_lab_TasselNet/Block_2_TN/Block_2_images_and_xml'\n",
    "block_0203 = '../../S_lab_TasselNet/Block_9_TN/Block_9_images_and_xml'\n",
    "block_0301 = '../../S_lab_TasselNet/Block_13_TN/Block_13_images_and_xml'\n",
    "\n",
    "train_blocks = [block_0101, block_0102, block_0203, block_0301]\n",
    "\n",
    "# valid data location\n",
    "block_0204 = '../../S_lab_TasselNet/Block_10_TN/Block_10_images_and_xml'\n",
    "\n",
    "valid_blocks = [block_0204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af534584-54d7-40b7-af33-caf36fdc2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the image and the xml files for the train data\n",
    "all_train_contents = []\n",
    "for item in train_blocks:\n",
    "    block_contents = os.listdir(item)\n",
    "    block_contents.sort()\n",
    "    print(len(block_contents))\n",
    "    all_train_contents.append(block_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5bc196-73f8-4b2a-bb85-a3154fe0aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to separate the image and the xml files - define a function\n",
    "def separate_img_xml(content_list):\n",
    "    image_files = []\n",
    "    xml_files = []\n",
    "    for file in content_list:\n",
    "        if file.split(\".\")[-1] == 'jpeg':\n",
    "            image_files.append(file)\n",
    "            image_files.sort()\n",
    "        else:\n",
    "            xml_files.append(file)\n",
    "            xml_files.sort()\n",
    "    return(image_files, xml_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f03ba-c89f-46d9-9056-cb5f88479ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files_11, xml_files_11 = separate_img_xml(all_train_contents[0])\n",
    "img_files_12, xml_files_12 = separate_img_xml(all_train_contents[1])\n",
    "img_files_23, xml_files_23 = separate_img_xml(all_train_contents[2])\n",
    "img_files_31, xml_files_31 = separate_img_xml(all_train_contents[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b95621-6d65-4f0f-a0b3-4cda7a7cb59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(img_files_11), len(xml_files_11))\n",
    "print(len(img_files_12), len(xml_files_12))\n",
    "print(len(img_files_23), len(xml_files_23))\n",
    "print(len(img_files_31), len(xml_files_31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c00829-81ac-4990-bf34-3bb6212e6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the annotated and not annotated images\n",
    "def separate_annot_and_not(img_list, xml_list):\n",
    "    image_names = [name.split(\".\")[0] for name in xml_list]\n",
    "    annotated_images = []\n",
    "    not_annotated_images = []\n",
    "    for file in img_list:\n",
    "        if file.split('.')[0] in image_names:\n",
    "            annotated_images.append(file)\n",
    "        else:\n",
    "            not_annotated_images.append(file)\n",
    "    return(annotated_images, not_annotated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bb059-f5a1-4413-9122-ab11ea2e4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_img_11, not_annot_img_11 = separate_annot_and_not(img_files_11, xml_files_11)\n",
    "annot_img_12, not_annot_img_12 = separate_annot_and_not(img_files_12, xml_files_12)\n",
    "annot_img_23, not_annot_img_23 = separate_annot_and_not(img_files_23, xml_files_23)\n",
    "annot_img_31, not_annot_img_31 = separate_annot_and_not(img_files_31, xml_files_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b7616-3611-4142-b58a-e9b93e4f9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separete the vertically and horizontally annotated images\n",
    "def separate_annnot_vertical_horizontal(annotated_images, all_xml_files, n):\n",
    "    annot_vertical_images = annotated_images[:n]\n",
    "    annot_vertical_xmls = all_xml_files[:n]\n",
    "    annot_horizontal_images = annotated_images[n:]\n",
    "    annot_horizontal_xmls = all_xml_files[n:]\n",
    "    return(annot_vertical_images, annot_vertical_xmls, annot_horizontal_images, annot_horizontal_xmls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcc773-5fe7-4202-bb1f-4c04222f98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_vertical_images_11, annot_vertical_xmls_11, annot_horizontal_images_11, annot_horizontal_xmls_11 = separate_annnot_vertical_horizontal(annot_img_11, \n",
    "                                                                                                                                             xml_files_11, 6)\n",
    "annot_vertical_images_12, annot_vertical_xmls_12, annot_horizontal_images_12, annot_horizontal_xmls_12 = separate_annnot_vertical_horizontal(annot_img_12, \n",
    "                                                                                                                                             xml_files_12, 6)\n",
    "annot_vertical_images_23, annot_vertical_xmls_23, annot_horizontal_images_23, annot_horizontal_xmls_23 = separate_annnot_vertical_horizontal(annot_img_23,\n",
    "                                                                                                                                             xml_files_23, 6)\n",
    "annot_vertical_images_31, annot_vertical_xmls_31, annot_horizontal_images_31, annot_horizontal_xmls_31 = separate_annnot_vertical_horizontal(annot_img_31, \n",
    "                                                                                                                                             xml_files_31, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b8f93-7369-45b6-b7f1-eba7b3119237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the annotated horizontal and vertical file names match\n",
    "def check_annot(annot_vertical_images, annot_vertical_xmls, annot_horizontal_images, annot_horizontal_xmls):\n",
    "    # are the vertical image and xml files matching\n",
    "    annot_vertical = []\n",
    "    for i in range(len(annot_vertical_images)):\n",
    "        if annot_vertical_images[i].split(\".\")[0] == annot_vertical_xmls[i].split(\".\")[0]:\n",
    "            annot_vertical.append(1)\n",
    "    print(len(annot_vertical), len(annot_vertical_images))\n",
    "\n",
    "    annot_horizontal = []\n",
    "    # are the horizontally annotated images and the xml files the same?\n",
    "    for j in range(len(annot_horizontal_images)):\n",
    "        if annot_horizontal_images[i].split(\".\")[0] == annot_horizontal_xmls[i].split(\".\")[0]:\n",
    "            annot_horizontal.append(1)\n",
    "    print(len(annot_horizontal), len(annot_horizontal_images))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7961d6-3d65-48ca-844e-e5ef916500cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_annot(annot_vertical_images_11, annot_vertical_xmls_11, annot_horizontal_images_11, annot_horizontal_xmls_11))\n",
    "print(check_annot(annot_vertical_images_12, annot_vertical_xmls_12, annot_horizontal_images_12, annot_horizontal_xmls_12))\n",
    "print(check_annot(annot_vertical_images_23, annot_vertical_xmls_23, annot_horizontal_images_23, annot_horizontal_xmls_23))\n",
    "print(check_annot(annot_vertical_images_31, annot_vertical_xmls_31, annot_horizontal_images_31, annot_horizontal_xmls_31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77383b3-51f0-4eb4-95d3-daf59daa5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now store the images as npy files? - Keep an eye out to see if this is really necessary\n",
    "\n",
    "# define a function to store the images as npy files - need two functions for vertical and horizontal separately - beacuse all images are read as horizontal images else.\n",
    "def store_images_as_np_arrays_vertical(img_old_path, img_name, img_store_path):\n",
    "    # join the path\n",
    "    image_path = os.path.join(img_old_path, img_name)\n",
    "    # read the image\n",
    "    read_image = plt.imread(image_path)\n",
    "    read_image = ndimage.rotate(read_image, 270)\n",
    "    image_size = read_image.shape\n",
    "    # show the image\n",
    "    plt.imshow(read_image)\n",
    "    plt.show()\n",
    "    # save the image in new location\n",
    "    np.save(img_store_path + '/' + img_name.split(\".\")[0] + '.npy', read_image)\n",
    "    return(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c74150-0d34-4d9a-a099-953964064f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_vertical_images_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec7122-3ce5-4f89-a820-04ffd622b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = 'Train_TN_data/vertical_annotated_images_as_np_arrays/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d63a18-7f76-45db-bcdf-af0a3c43a2e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_blocks = [block_0101, block_0102, block_0203, block_0301]\n",
    "# blk 11\n",
    "im_shapes_annot_vertical_11 = []\n",
    "for item in annot_vertical_images_11:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0101, item, store_path)\n",
    "    im_shapes_annot_vertical_11.append(img_shape)\n",
    "print(\"Block 11 done!\")\n",
    "\n",
    "# blk 12\n",
    "im_shapes_annot_vertical_12 = []\n",
    "for item in annot_vertical_images_12:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0102, item, store_path)\n",
    "    im_shapes_annot_vertical_12.append(img_shape)\n",
    "print(\"Block 12 done!\")\n",
    "\n",
    "# blk 23\n",
    "im_shapes_annot_vertical_23 = []\n",
    "for item in annot_vertical_images_23:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0203, item, store_path)\n",
    "    im_shapes_annot_vertical_23.append(img_shape)\n",
    "print(\"Block 23 done!\")\n",
    "\n",
    "# blk 31\n",
    "im_shapes_annot_vertical_31 = []\n",
    "for item in annot_vertical_images_31:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0301, item, store_path)\n",
    "    im_shapes_annot_vertical_31.append(img_shape)\n",
    "print(\"Block 31 done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8aa272-8163-40a6-beeb-9fda6b2d9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(im_shapes_annot_vertical_11)\n",
    "print(im_shapes_annot_vertical_12)\n",
    "print(im_shapes_annot_vertical_23)\n",
    "print(im_shapes_annot_vertical_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733dea85-749c-453b-9bb0-3ae65237af8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store the vertically not-annotated images\n",
    "store_path = 'Train_TN_data/vertical_not_annotated_images_as_np_arrays/'\n",
    "\n",
    "# blk 11\n",
    "im_shapes_notannot_vertical_11 = []\n",
    "for item in not_annot_img_11:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0101, item, store_path)\n",
    "    im_shapes_notannot_vertical_11.append(img_shape)\n",
    "print(\"Block 11 done!\")\n",
    "\n",
    "# blk 12\n",
    "im_shapes_notannot_vertical_12 = []\n",
    "for item in not_annot_img_12:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0102, item, store_path)\n",
    "    im_shapes_notannot_vertical_12.append(img_shape)\n",
    "print(\"Block 12 done!\")\n",
    "\n",
    "# blk 23\n",
    "im_shapes_notannot_vertical_23 = []\n",
    "for item in not_annot_img_23:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0203, item, store_path)\n",
    "    im_shapes_notannot_vertical_23.append(img_shape)\n",
    "print(\"Block 23 done!\")\n",
    "\n",
    "# blk 31 - the first image needs to be rotated here\n",
    "im_shapes_notannot_vertical_31 = []\n",
    "for item in not_annot_img_31[1:]:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0301, item, store_path)\n",
    "    im_shapes_notannot_vertical_31.append(img_shape)\n",
    "print(\"Block 31 done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59906e6a-79a5-4c16-85db-4d45b435ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outlier image in block 31 - the first vertical na image\n",
    "def store_images_as_np_arrays_vertical_outlier(img_old_path, img_name, img_store_path):\n",
    "    # join the path\n",
    "    image_path = os.path.join(img_old_path, img_name)\n",
    "    # read the image\n",
    "    read_image = plt.imread(image_path)\n",
    "    read_image = ndimage.rotate(read_image, 90)\n",
    "    image_size = read_image.shape\n",
    "    # show the image\n",
    "    plt.imshow(read_image)\n",
    "    plt.show()\n",
    "    # save the image in new location\n",
    "    np.save(img_store_path + '/' + img_name.split(\".\")[0] + '.npy', read_image)\n",
    "    return(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6312389-045a-49a0-a33b-7fa93901b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape_na_0_31 = store_images_as_np_arrays_vertical_outlier(block_0301, not_annot_img_31[0], store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe83e18-ce23-4bb3-9b9d-74818fe1f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to store horizontal images\n",
    "def store_images_as_np_arrays_horizontal(img_old_path, img_name, img_store_path):\n",
    "    # join the path\n",
    "    image_path = os.path.join(img_old_path, img_name)\n",
    "    # read the image\n",
    "    read_image = plt.imread(image_path)\n",
    "    image_size = read_image.shape\n",
    "    # show the image\n",
    "    plt.imshow(read_image)\n",
    "    plt.show()\n",
    "    # save the image in new location\n",
    "    np.save(img_store_path + '/' + img_name.split(\".\")[0] + '.npy', read_image)\n",
    "    return(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007fc7c-009f-4278-b228-9f0f6221b04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store the horizontally annotated images\n",
    "store_path = \"Train_TN_data/horizontal_annotated_images_as_np_arrays\"\n",
    "\n",
    "# blk 11 - the first horizontal image has a problem, needs to be rotated\n",
    "im_shapes_annot_horizontal_11 = []\n",
    "for item in annot_horizontal_images_11:\n",
    "    img_shape = store_images_as_np_arrays_horizontal(block_0101, item, store_path)\n",
    "    im_shapes_annot_horizontal_11.append(img_shape)\n",
    "print(\"Block 11 done!\")\n",
    "\n",
    "# blk 12\n",
    "im_shapes_annot_horizontal_12 = []\n",
    "for item in annot_horizontal_images_12:\n",
    "    img_shape = store_images_as_np_arrays_horizontal(block_0102, item, store_path)\n",
    "    im_shapes_annot_horizontal_12.append(img_shape)\n",
    "print(\"Block 12 done!\")\n",
    "\n",
    "# blk 23\n",
    "im_shapes_annot_horizontal_23 = []\n",
    "for item in annot_horizontal_images_23:\n",
    "    img_shape = store_images_as_np_arrays_horizontal(block_0203, item, store_path)\n",
    "    im_shapes_annot_horizontal_23.append(img_shape)\n",
    "print(\"Block 23 done!\")\n",
    "\n",
    "# blk 31\n",
    "im_shapes_annot_horizontal_31 = []\n",
    "for item in annot_horizontal_images_31:\n",
    "    img_shape = store_images_as_np_arrays_horizontal(block_0301, item, store_path)\n",
    "    im_shapes_annot_horizontal_31.append(img_shape)\n",
    "print(\"Block 31 done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304890f-3fc1-4c7c-b44c-bed6944f3777",
   "metadata": {},
   "source": [
    "create density maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ba4a4-fbdb-418c-9332-e33bd39d0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe copy all the xml files corresponding to the train data into a single location? That would be easier with data prep I believe\n",
    "# train_blocks\n",
    "# train_blocks = [block_0101, block_0102, block_0203, block_0301]\n",
    "new_store_path = 'Train_TN_data/train_xml_files'\n",
    "# blk 11\n",
    "for file in xml_files_11:\n",
    "    joined_path = os.path.join(block_0101, file)\n",
    "    # store the file in new path\n",
    "    shutil.copy(joined_path, new_store_path)\n",
    "\n",
    "# blk 12\n",
    "for file in xml_files_12:\n",
    "    joined_path = os.path.join(block_0102, file)\n",
    "    # store the file in new path\n",
    "    shutil.copy(joined_path, new_store_path)\n",
    "\n",
    "# blk 23\n",
    "for file in xml_files_23:\n",
    "    joined_path = os.path.join(block_0203, file)\n",
    "    # store the file in new path\n",
    "    shutil.copy(joined_path, new_store_path)\n",
    "\n",
    "# blk 31\n",
    "for file in xml_files_31:\n",
    "    joined_path = os.path.join(block_0301, file)\n",
    "    # store the file in new path\n",
    "    shutil.copy(joined_path, new_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d98b58-0b82-45ad-ab21-d2bb5d3b0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vertically annotated images\n",
    "def get_density_maps_vertical(file_name, image_path, xml_path, save_density_path):\n",
    "    xml_file = file_name + '.xml'\n",
    "    xml_file_path = os.path.join(xml_path, xml_file)\n",
    "\n",
    "    # Get coords from the xml file\n",
    "    # parse the xml file\n",
    "    parsed_file = ET.parse(xml_file_path)\n",
    "    # get the roots\n",
    "    root = parsed_file.getroot()\n",
    "    # get the roots here\n",
    "    coords = []\n",
    "    for child in root:\n",
    "        for i in child:\n",
    "            for j in i:\n",
    "                coords.append(int(j.text))\n",
    "    \n",
    "    # chunk the points into sets of 4 - these are the coordinates of the bounding boxes\n",
    "    points_tupples = []\n",
    "    for i in range(0, len(coords), 4):\n",
    "        points_tupples.append(coords[i:i + 4])\n",
    "\n",
    "    # make a dataframe with these points\n",
    "    coords_df = pd.DataFrame(points_tupples, columns = [\"bleft_x\", \"bleft_y\", \"tright_x\", \"tright_y\"])\n",
    "\n",
    "    # compute the number of tassels in each image\n",
    "    no_of_tassels = len(points_tupples)\n",
    "\n",
    "    # compute the mid coordinates\n",
    "    coords_df[\"mid_x\"] = (round(0.5*(coords_df[\"bleft_x\"] + coords_df[\"tright_x\"]))).astype(int)\n",
    "    coords_df[\"mid_y\"] = (round(0.5*(coords_df[\"bleft_y\"] + coords_df[\"tright_y\"]))).astype(int)\n",
    "\n",
    "    # extract the mid cordinates\n",
    "    mid_coords = coords_df[[\"mid_x\", \"mid_y\"]]\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # cap the coords at the max height and width values\n",
    "    mid_coords.loc[mid_coords['mid_x'] > 768, 'mid_x'] = 767\n",
    "    mid_coords.loc[mid_coords['mid_y'] > 1024, 'mid_y'] = 1023\n",
    "    \n",
    "\n",
    "    # plot the bounding boxes on images\n",
    "    # get image name and path\n",
    "    image_name = file_name + '.npy'\n",
    "    imge_file_path = os.path.join(image_path, image_name)\n",
    "    # read the image\n",
    "    read_image = np.load(imge_file_path)\n",
    "\n",
    "    # check the shape of the read image\n",
    "    read_image_shape = read_image.shape\n",
    "    #  plot the bounding boxes on the image\n",
    "    for points in points_tupples:\n",
    "        annotated_image = cv2.rectangle(read_image, (points[0],points[1]), (points[2],points[3]), color = (255,0,0), thickness = 2)\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.show()\n",
    "\n",
    "    # plot the mid points on the image\n",
    "    coords_list = mid_coords.values.tolist()\n",
    "    # read the image again\n",
    "    read_image_again = np.load(imge_file_path)\n",
    "    # read_image_again = ndimage.rotate(read_image_again, 270)\n",
    "    # draw the circles on image\n",
    "    for i in coords_list:\n",
    "        image_with_mids = cv2.circle(read_image_again, i, radius=5, color=(255, 0, 0), thickness=-1)\n",
    "    # look at the annotated image\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(image_with_mids)\n",
    "    plt.show()\n",
    "\n",
    "    # also try creating the density map here\n",
    "    # first create the empty maps\n",
    "    np_image = np.zeros((read_image_shape[0], read_image_shape[1]))\n",
    "    # get the dot maps\n",
    "    for point in coords_list:\n",
    "        np_image[point[1], point[0]] = 1\n",
    "    # plot the image\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(np_image, cmap = \"Greys\")\n",
    "    plt.show()\n",
    "\n",
    "    # now define the kernel and run the convolution\n",
    "    one_d_kerenel = cv2.getGaussianKernel(50,5)\n",
    "    two_d_kernel = np.multiply(one_d_kerenel.T, one_d_kerenel)\n",
    "\n",
    "    # Shape of the 2D kernel\n",
    "    twoD_shape = two_d_kernel.shape\n",
    "        \n",
    "    # do the convolution\n",
    "    convolution = ndimage.convolve(np_image, two_d_kernel)\n",
    "        \n",
    "    # plot the density map\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(convolution, cmap = \"Greys\")\n",
    "    plt.show()\n",
    "        \n",
    "    # get the sums of the images\n",
    "    img_sum = np.sum(convolution)\n",
    "\n",
    "    # save the density map\n",
    "    np.save(save_density_path + '/' + file_name + '_density_map.npy', convolution)\n",
    "\n",
    "    return(file_name, read_image_shape, no_of_tassels, img_sum, convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75744e85-60c3-4919-8492-2c20bcfc95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the outputs from this function\n",
    "\n",
    "image_path = 'Train_TN_data/vertical_annotated_images_as_np_arrays/'\n",
    "xml_path = 'Train_TN_data/train_xml_files'\n",
    "save_density_path = image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc8b4cf-74ba-4179-b55d-fe4477c640ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294610a-2ddf-402a-8c29-de0acd09bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_v = [file.split(\".\")[0] for file in os.listdir(image_path) if file.split(\".\")[0][-3:] != 'map']\n",
    "file_names_v.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664e80f-cf3e-48ee-8604-d097f7f23507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be61c51-9914-43f2-95e1-383d84a236c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "names_all_va = []\n",
    "shapes_all_va = []\n",
    "tasselCounts_all_va = []\n",
    "tasselDensities_all_va = []\n",
    "dense_maps_all_va = []\n",
    "for file_name in file_names_v:\n",
    "    name_va, shape_va, tassels_va, tassel_dense_va, dense_map_va = get_density_maps_vertical(file_name, image_path, xml_path, save_density_path)\n",
    "    names_all_va.append(name_va)\n",
    "    shapes_all_va.append(shape_va)\n",
    "    tasselCounts_all_va.append(tassels_va)\n",
    "    tasselDensities_all_va.append(tassel_dense_va)\n",
    "    dense_maps_all_va.append(dense_map_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e94cb4-ecee-456f-aece-f2a10812c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few sanity checks\n",
    "np.mean(tasselCounts_all_va == np.round(tasselDensities_all_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149810f-db6b-48b4-99e1-96e39000353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the true counts\n",
    "True_tasselcounts_df_vertical_annot = pd.DataFrame(zip(names_all_va, tasselCounts_all_va), columns = ['name', 'true_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001cb84-d1c2-41e0-a703-441a1a488936",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_tasselcounts_df_vertical_annot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689b766-9a01-4dae-bf38-f65e3fb8f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_tasselcounts_df_vertical_annot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad432121-e9c1-4905-b12e-1eb4c78cec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the true counts\n",
    "True_tasselcounts_df_vertical_annot.to_csv(\"Train_TN_data/True_tassel_counts/vertical_annotated_true_counts.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48447bcb-082b-4ff8-ac43-00929eed8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create density maps for the horizontally annotated images\n",
    "def get_density_maps_horizontal(file_name, image_path, xml_path, save_density_path):\n",
    "    xml_file = file_name + '.xml'\n",
    "    xml_file_path = os.path.join(xml_path, xml_file)\n",
    "\n",
    "    # Get coords from the xml file\n",
    "    # parse the xml file\n",
    "    parsed_file = ET.parse(xml_file_path)\n",
    "    # get the roots\n",
    "    root = parsed_file.getroot()\n",
    "    # get the roots here\n",
    "    coords = []\n",
    "    for child in root:\n",
    "        for i in child:\n",
    "            for j in i:\n",
    "                coords.append(int(j.text))\n",
    "    \n",
    "    # chunk the points into sets of 4 - these are the coordinates of the bounding boxes\n",
    "    points_tupples = []\n",
    "    for i in range(0, len(coords), 4):\n",
    "        points_tupples.append(coords[i:i + 4])\n",
    "\n",
    "    # make a dataframe with these points\n",
    "    coords_df = pd.DataFrame(points_tupples, columns = [\"bleft_x\", \"bleft_y\", \"tright_x\", \"tright_y\"])\n",
    "\n",
    "    # compute the number of tassels in each image\n",
    "    no_of_tassels = len(points_tupples)\n",
    "\n",
    "    # compute the mid coordinates\n",
    "    coords_df[\"mid_x\"] = (round(0.5*(coords_df[\"bleft_x\"] + coords_df[\"tright_x\"]))).astype(int)\n",
    "    coords_df[\"mid_y\"] = (round(0.5*(coords_df[\"bleft_y\"] + coords_df[\"tright_y\"]))).astype(int)\n",
    "\n",
    "    # extract the mid cordinates\n",
    "    mid_coords = coords_df[[\"mid_x\", \"mid_y\"]]\n",
    "    # cap the coords at the max height and width values\n",
    "    mid_coords.loc[mid_coords['mid_x'] > 1024, 'mid_x'] = 1023\n",
    "    mid_coords.loc[mid_coords['mid_y'] > 768, 'mid_y'] = 767\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # plot the bounding boxes on images\n",
    "    # get image name and path\n",
    "    image_name = file_name + '.npy'\n",
    "    imge_file_path = os.path.join(image_path, image_name)\n",
    "    # read the image\n",
    "    read_image = np.load(imge_file_path)\n",
    "    # check the shape of the read image\n",
    "    read_image_shape = read_image.shape\n",
    "    #  plot the bounding boxes on the image\n",
    "    for points in points_tupples:\n",
    "        annotated_image = cv2.rectangle(read_image, (points[0],points[1]), (points[2],points[3]), color = (255,0,0), thickness = 2)\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.show()\n",
    "\n",
    "    # plot the mid points on the image\n",
    "    coords_list = mid_coords.values.tolist()\n",
    "    # read the image again\n",
    "    read_image_again = np.load(imge_file_path)\n",
    "    # draw the circles on image\n",
    "    for i in coords_list:\n",
    "        image_with_mids = cv2.circle(read_image_again, i, radius=5, color=(255, 0, 0), thickness=-1)\n",
    "    # look at the annotated image\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(image_with_mids)\n",
    "    plt.show()\n",
    "\n",
    "    # also try creating the density map here\n",
    "    # first create the empty maps\n",
    "    np_image = np.zeros((read_image_shape[0], read_image_shape[1]))\n",
    "    # get the dot maps\n",
    "    for point in coords_list:\n",
    "        np_image[point[1], point[0]] = 1\n",
    "    # plot the image\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(np_image, cmap = \"Greys\")\n",
    "    plt.show()\n",
    "\n",
    "    # now define the kernel and run the convolution\n",
    "    one_d_kerenel = cv2.getGaussianKernel(50,5)\n",
    "    two_d_kernel = np.multiply(one_d_kerenel.T, one_d_kerenel)\n",
    "\n",
    "    # Shape of the 2D kernel\n",
    "    twoD_shape = two_d_kernel.shape\n",
    "        \n",
    "    # do the convolution\n",
    "    convolution = ndimage.convolve(np_image, two_d_kernel)\n",
    "        \n",
    "    # plot the density map\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(convolution, cmap = \"Greys\")\n",
    "    plt.show()\n",
    "        \n",
    "    # get the sums of the images\n",
    "    img_sum = np.sum(convolution)\n",
    "\n",
    "    # save the density map\n",
    "    np.save(save_density_path + '/' + file_name + '_density_map.npy', convolution)\n",
    "\n",
    "    return(file_name, read_image_shape, no_of_tassels, img_sum, convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8412aa-17cd-4b3d-a2b5-68b9393caa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the outputs from this function\n",
    "image_path_h = \"Train_TN_data/horizontal_annotated_images_as_np_arrays\"\n",
    "xml_path_h = 'Train_TN_data/train_xml_files'\n",
    "save_density_path_h = image_path_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc98083-5375-40e7-bf0f-83ea9e8feac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_h = [file.split(\".\")[0] for file in os.listdir(image_path_h) if file.split(\".\")[0][-3:] != 'map']\n",
    "file_names_h.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d508a0a-4924-499c-a0d0-4437a31dba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8b15a-beb1-423b-998f-26792a04781f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "names_all_h = []\n",
    "shapes_all_h = []\n",
    "tasselCounts_all_h = []\n",
    "tasselDensities_all_h = []\n",
    "dense_maps_all_h = []\n",
    "for file_h in file_names_h:\n",
    "    name_h, shape_h, tassels_h, tassel_dense_h, dense_map_h = get_density_maps_horizontal(file_h, image_path_h, xml_path_h, save_density_path_h)\n",
    "    names_all_h.append(name_h)\n",
    "    shapes_all_h.append(shape_h)\n",
    "    tasselCounts_all_h.append(tassels_h)\n",
    "    tasselDensities_all_h.append(tassel_dense_h)\n",
    "    dense_maps_all_h.append(dense_map_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b2fdb-1163-45e0-9000-4b67cfa7787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few sanity checks\n",
    "np.mean(tasselCounts_all_h == np.round(tasselDensities_all_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d34156-ec21-4f9d-84dd-fc40674f6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the true counts\n",
    "True_tasselcounts_df_horizontal_annot = pd.DataFrame(zip(names_all_h, tasselCounts_all_h), columns = ['name', 'true_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315d27f-9fd6-45e6-a200-6981654d2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_tasselcounts_df_horizontal_annot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efaa150-39f2-49af-82b6-136ac59a99c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_tasselcounts_df_horizontal_annot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f4ad9-b4dc-4394-985c-f7b192ae9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the true counts\n",
    "True_tasselcounts_df_horizontal_annot.to_csv(\"Train_TN_data/True_tassel_counts/horizontal_annotated_true_counts.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf2066-350d-42de-9343-719cfd7461df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vertically annotated images\n",
    "def get_density_maps_not_annotated(file_name, image_path, save_density_path):\n",
    "    \n",
    "    # plot the bounding boxes on images\n",
    "    # get image name and path\n",
    "    image_name = file_name + '.npy'\n",
    "    imge_file_path = os.path.join(image_path, image_name)\n",
    "    # read the image\n",
    "    read_image = np.load(imge_file_path)\n",
    "\n",
    "    plt.imshow(read_image)\n",
    "    plt.show()\n",
    "\n",
    "    read_image_shape = read_image.shape\n",
    "    \n",
    "    # also try creating the density map here\n",
    "    np_image = np.zeros((read_image_shape[0], read_image_shape[1]))\n",
    "\n",
    "    # save the density map\n",
    "    np.save(save_density_path + '/' + file_name + '_density_map.npy', np_image)\n",
    "\n",
    "    img_sum = np.sum(np_image)\n",
    "\n",
    "    return(file_name, read_image_shape, img_sum, np_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f9615-8d45-4769-9b05-0f23a27272ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_vn = 'Train_TN_data/vertical_not_annotated_images_as_np_arrays/'\n",
    "save_density_path_vn = image_path_vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f22f29-f37e-4b2a-85e2-b2af61937e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_vn = [file.split(\".\")[0] for file in os.listdir(image_path_vn) if file.split(\".\")[0][-3:] != 'map']\n",
    "file_names_vn.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d252e-036a-41e6-9253-538277e768e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_names_vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58f55e-02ff-4c2a-974e-82317867b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get the density maps for all vertically annotated image files\n",
    "names_all_vn = []\n",
    "shapes_all_vn = []\n",
    "tasselDensities_all_vn = []\n",
    "dense_maps_all_vn = []\n",
    "for file_vn in file_names_vn:\n",
    "    name_vn, shape_vn, tassel_dense_vn, dense_map_vn = get_density_maps_not_annotated(file_vn, image_path_vn, save_density_path_vn)\n",
    "    names_all_vn.append(name_vn)\n",
    "    shapes_all_vn.append(shape_vn)\n",
    "    tasselDensities_all_vn.append(tassel_dense_vn)\n",
    "    dense_maps_all_vn.append(dense_map_vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc312bf-c404-4653-b0b4-36185e2cc095",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vertical_not_annotated_df = pd.DataFrame(zip(names_all_vn, tasselDensities_all_vn), columns = ['name', 'true_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6c2ce-e154-421d-9523-cf390a4a8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vertical_not_annotated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5c175-b4af-4046-bea4-d7489fe8e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vertical_not_annotated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832a066-435a-49f2-9a75-b5c5b832ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the true counts\n",
    "Vertical_not_annotated_df.to_csv(\"Train_TN_data/True_tassel_counts/vertical_not_annotated_true_counts.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539ae71-df1c-4aa8-ae0b-c3e9e63374f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all the image np and density xml files to a single location\n",
    "\n",
    "va_images_dense = 'Train_TN_data/vertical_annotated_images_as_np_arrays/'\n",
    "vn_images_dense = 'Train_TN_data/vertical_not_annotated_images_as_np_arrays/'\n",
    "h_images_dense = 'Train_TN_data/horizontal_annotated_images_as_np_arrays/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02971de-c8e0-467e-9278-ad10d3b4c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shutil.copytree(va_images_dense, 'Train_TN_data/train_all_image_density_files/', dirs_exist_ok=True)\n",
    "shutil.copytree(vn_images_dense, 'Train_TN_data/train_all_image_density_files/', dirs_exist_ok=True)\n",
    "shutil.copytree(h_images_dense, 'Train_TN_data/train_all_image_density_files/', dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391a6f6-fa2f-4819-9ad6-8dc644b8b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sub windows and counts - but does not seem like these were stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b588b3-93a0-4f3d-9408-e5ec0fecaf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subwindows_and_counts(image, numpy_folder, stride = 8, kernel_size = 32):\n",
    "    im_name = image.split(\".\")[0]\n",
    "    im_file = im_name + '.npy'\n",
    "    count_file = im_name + '_density_map.npy'\n",
    "    # load the image and the count numpy files\n",
    "    loaded_im_file = np.load(numpy_folder + '/' + im_file)\n",
    "    loaded_count_file = np.load(numpy_folder + '/' + count_file)\n",
    "        \n",
    "    # create the subwindows and counts as follows\n",
    "    img_height = loaded_im_file.shape[0]\n",
    "    img_width = loaded_im_file.shape[1]\n",
    "    \n",
    "    density_sums = []\n",
    "    catch_image = []\n",
    "    for i in  range(0, img_height, stride):\n",
    "        for j in range(0, img_width, stride):\n",
    "            sub_window = loaded_im_file[i: i + kernel_size, j : j + kernel_size,:]\n",
    "            density = loaded_count_file[i: i + kernel_size, j : j + kernel_size]\n",
    "            dense_sum = np.sum(density)\n",
    "            density_sums.append(dense_sum)\n",
    "            sub_window = resize(sub_window, (32, 32,3))\n",
    "            catch_image.append(sub_window)\n",
    "\n",
    "    # save the combined subwindows and counts\n",
    "    return(catch_image,density_sums, im_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16778eef-c6e8-4124-bf70-6b00786027d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do this for a sample and then in the loop\n",
    "train_files_path = \"Train_TN_data/train_all_image_density_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1dc01-137a-4b42-b614-45acc94b629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_im_and_map_contents = os.listdir(train_files_path)\n",
    "\n",
    "# sort these - ALWAYS sort these as the order is always messed up on HCC\n",
    "train_im_and_map_contents.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b459b6a-3911-4fdb-8cbd-d93f46204381",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_im_and_map_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39738f-cf5f-4cf8-9a0e-c90cfbf039ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the names of the image (npy files)\n",
    "train_im_names = [item for item in train_im_and_map_contents if item.split(\".\")[0][-3:] != 'map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce4dfa-0bf2-4555-90eb-7e9c21d6618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many? should be 32\n",
    "len(train_im_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf8b0f-4195-4007-8e16-60fa0a6b889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_im_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82517aee-8968-4d53-887f-41edd067e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # create the subwindows for all train data\n",
    "# catch_all_image_subwindows_train = []\n",
    "# catch_all_dense_subwindows_train = []\n",
    "# catch_train_names = []\n",
    "# for image in train_im_names:\n",
    "#     train_ims, train_maps, train_names = create_subwindows_and_counts(image, train_files_path, stride = 8, kernel_size = 32)\n",
    "#     catch_all_image_subwindows_train.append(train_ims)\n",
    "#     catch_all_dense_subwindows_train.append(train_maps)\n",
    "#     catch_train_names.append(train_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8886e82-b39a-4c57-a31c-67efcac01e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sanity check names\n",
    "# np.mean(train_im_names == catch_train_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0144b4-e05c-4bdf-964e-718f0e553828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that stacking is not done here, but done separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57bfa8-117a-4ad8-b8a1-9f2d94621489",
   "metadata": {},
   "source": [
    "Prepare validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b080a3-c704-4640-83a0-959b70ca310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the image and the xml files for the train data\n",
    "all_valid_contents = os.listdir(block_0204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b033cd-fdf9-4618-bdfe-2f6177d19a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate image and xml files\n",
    "img_files_24, xml_files_24 = separate_img_xml(all_valid_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21471e57-bb3b-4151-8d53-b4f2174759a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml_files_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea7e35-0f93-4330-84bc-633d90fd84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate annotated and not annotated images\n",
    "annot_img_24, not_annot_img_24 = separate_annot_and_not(img_files_24, xml_files_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74999e8c-ae45-4272-89a8-31446cd0f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_annot_img_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c686e91-b8e4-4ea8-9dfc-b5eda4c4098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate vertical and horizontal annotated images\n",
    "annot_vertical_images_24, annot_vertical_xmls_24, annot_horizontal_images_24, annot_horizontal_xmls_24 = separate_annnot_vertical_horizontal(annot_img_24, \n",
    "                                                                                                                                             xml_files_24, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6e1d2-e65c-4bb2-8a95-1ee51040ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shapes\n",
    "print(check_annot(annot_vertical_images_24, annot_vertical_xmls_24, annot_horizontal_images_24, annot_horizontal_xmls_24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14931c12-7cb1-4db9-b440-4d6a9ad6162f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store the vertically annotated images - validation data\n",
    "store_path = \"Train_TN_data/vertical_annotated_images_as_np_arrays_valid\"\n",
    "\n",
    "# blk 24\n",
    "im_shapes_annot_vertical_24 = []\n",
    "for item in annot_vertical_images_24:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0204, item, store_path)\n",
    "    im_shapes_annot_vertical_24.append(img_shape)\n",
    "print(\"Block 24 done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6615344-fd2f-4c80-8686-ee54bfa541f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(im_shapes_annot_vertical_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9c660-e309-426b-b7b4-7e7424eb7a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store the vertically not-annotated images\n",
    "store_path = \"Train_TN_data/vertical_not_annotated_images_as_np_arrays_valid\"\n",
    "\n",
    "# blk 24\n",
    "im_shapes_notannot_vertical_24 = []\n",
    "for item in not_annot_img_24:\n",
    "    img_shape = store_images_as_np_arrays_vertical(block_0204, item, store_path)\n",
    "    im_shapes_notannot_vertical_24.append(img_shape)\n",
    "print(\"Block 24 done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164df86-98fa-43e8-b395-daad79513579",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(im_shapes_notannot_vertical_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87096612-14ec-41e3-98a0-ad42081528a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store the horizontally annotated images\n",
    "store_path = \"Train_TN_data/horizontal_annotated_images_as_np_arrays_valid\"\n",
    "\n",
    "# blk 11 - the first horizontal image has a problem, needs to be rotated\n",
    "im_shapes_annot_horizontal_24 = []\n",
    "for item in annot_horizontal_images_24:\n",
    "    img_shape = store_images_as_np_arrays_horizontal(block_0204, item, store_path)\n",
    "    im_shapes_annot_horizontal_24.append(img_shape)\n",
    "print(\"Block 24 done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042f469-47f3-45c5-9b70-29aea459e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the xml file paths in a single location\n",
    "new_store_path = 'Train_TN_data/valid_xml_files'\n",
    "# blk 11\n",
    "for file in xml_files_24:\n",
    "    joined_path = os.path.join(block_0204, file)\n",
    "    # store the file in new path\n",
    "    shutil.copy(joined_path, new_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e708fc8-c5d6-4b15-b77a-9cbf9583a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the outputs from this function\n",
    "image_path = \"Train_TN_data/vertical_annotated_images_as_np_arrays_valid/\"\n",
    "xml_path = 'Train_TN_data/valid_xml_files'\n",
    "save_density_path = image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cbe82-feae-4754-9fa7-6f4314592b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_v = [file.split(\".\")[0] for file in os.listdir(image_path) if file.split(\".\")[0][-3:] != 'map']\n",
    "file_names_v.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb9ba1-5460-47f5-ac49-7b59c0793c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0e11e-bccc-4498-989b-d00cf1491da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "names_all_va_v = []\n",
    "shapes_all_va_v = []\n",
    "tasselCounts_all_va_v = []\n",
    "tasselDensities_all_va_v = []\n",
    "dense_maps_all_va_v = []\n",
    "for file_name in file_names_v:\n",
    "    name_va, shape_va, tassels_va, tassel_dense_va, dense_map_va = get_density_maps_vertical(file_name, image_path, xml_path, save_density_path)\n",
    "    names_all_va_v.append(name_va)\n",
    "    shapes_all_va_v.append(shape_va)\n",
    "    tasselCounts_all_va_v.append(tassels_va)\n",
    "    tasselDensities_all_va_v.append(tassel_dense_va)\n",
    "    dense_maps_all_va_v.append(dense_map_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ef7de-efa3-479f-b275-6e0098c7171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few sanity checks\n",
    "np.mean(tasselCounts_all_va == np.round(tasselDensities_all_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa7222-b3a0-46ba-a285-f82cf93e471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the true counts\n",
    "True_tasselcounts_df_vertical_annot_valid = pd.DataFrame(zip(names_all_va_v, tasselCounts_all_va_v), columns = ['name', 'true_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c224e4-7152-435c-8ab3-66b671a5c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_tasselcounts_df_vertical_annot_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1fcdb-0247-4ea9-b15a-ad56ee11625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_tasselcounts_df_vertical_annot_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e8091-88d7-4dc4-b933-8bd5af9a9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the true counts\n",
    "True_tasselcounts_df_vertical_annot_valid.to_csv(\"Train_TN_data/True_tassel_counts/vertical_annotated_true_counts_valid.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8747f5b-8f45-4206-9e10-064e85c3ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the outputs from this function\n",
    "image_path_h = \"Train_TN_data/horizontal_annotated_images_as_np_arrays_valid/\"\n",
    "xml_path_h = 'Train_TN_data/valid_xml_files'\n",
    "save_density_path_h = image_path_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada505fd-96e6-4c4e-9567-8bca12cf2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_h = [file.split(\".\")[0] for file in os.listdir(image_path_h) if file.split(\".\")[0][-3:] != 'map']\n",
    "file_names_h.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea817b66-34bc-410f-a548-34c879d9af65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "names_all_h_v = []\n",
    "shapes_all_h_v = []\n",
    "tasselCounts_all_h_v = []\n",
    "tasselDensities_all_h_v = []\n",
    "dense_maps_all_h_v = []\n",
    "for file_h in file_names_h:\n",
    "    name_h, shape_h, tassels_h, tassel_dense_h, dense_map_h = get_density_maps_horizontal(file_h, image_path_h, xml_path_h, save_density_path_h)\n",
    "    names_all_h_v.append(name_h)\n",
    "    shapes_all_h_v.append(shape_h)\n",
    "    tasselCounts_all_h_v.append(tassels_h)\n",
    "    tasselDensities_all_h_v.append(tassel_dense_h)\n",
    "    dense_maps_all_h_v.append(dense_map_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef807e7-1d6c-48ac-a859-ef6ef07636c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few sanity checks\n",
    "np.mean(tasselCounts_all_h == np.round(tasselDensities_all_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a180519-48fb-4f40-acc8-433f7b3d5ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the true counts\n",
    "True_tasselcounts_df_horizontal_annot_valid = pd.DataFrame(zip(names_all_h_v, tasselCounts_all_h_v), columns = ['name', 'true_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec840d22-e2a7-40af-83a0-80ba7cc8b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_tasselcounts_df_horizontal_annot_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e379c-1e7e-45b0-8609-6fff00983aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_tasselcounts_df_horizontal_annot_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84b6f3-6040-4797-93da-5345312dea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the true counts\n",
    "True_tasselcounts_df_horizontal_annot_valid.to_csv(\"Train_TN_data/True_tassel_counts/horizontal_annotated_true_counts_valid.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a0933-d069-424d-80e4-93b990ddb476",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_vn = 'Train_TN_data/vertical_not_annotated_images_as_np_arrays_valid/'\n",
    "save_density_path_vn = image_path_vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb2f27-8a3f-42c5-9152-ee7cdc7b38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_vn = [file.split(\".\")[0] for file in os.listdir(image_path_vn) if file.split(\".\")[0][-3:] != 'map']\n",
    "file_names_vn.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfe11d-64b3-4e65-9701-b665a576a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_names_vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862b29d-254d-41ac-b732-6cd5e5d7b4f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get the density maps for all vertically annotated image files\n",
    "names_all_vn_v = []\n",
    "shapes_all_vn_v = []\n",
    "tasselDensities_all_vn_v = []\n",
    "dense_maps_all_vn_v = []\n",
    "for file_vn in file_names_vn:\n",
    "    name_vn, shape_vn, tassel_dense_vn, dense_map_vn = get_density_maps_not_annotated(file_vn, image_path_vn, save_density_path_vn)\n",
    "    names_all_vn_v.append(name_vn)\n",
    "    shapes_all_vn_v.append(shape_vn)\n",
    "    tasselDensities_all_vn_v.append(tassel_dense_vn)\n",
    "    dense_maps_all_vn_v.append(dense_map_vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101ebdd-f36b-4b75-936a-74c8aeb0ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vertical_not_annotated_df_valid = pd.DataFrame(zip(names_all_vn_v, tasselDensities_all_vn_v), columns = ['name', 'true_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b149df2-bb0c-461e-ab29-5bef1f2cc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vertical_not_annotated_df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a63e10-d697-4169-a5c9-e49e75acb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vertical_not_annotated_df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c26129-6dbf-4a3d-a378-ac494e2c1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the true counts\n",
    "Vertical_not_annotated_df.to_csv(\"Train_TN_data/True_tassel_counts/vertical_not_annotated_true_countsvalid.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5681a-cbc6-4788-89e1-aa754c4e8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all the image np and density xml files to a single location\n",
    "\n",
    "va_images_dense = 'Train_TN_data/vertical_annotated_images_as_np_arrays_valid/'\n",
    "vn_images_dense = 'Train_TN_data/vertical_not_annotated_images_as_np_arrays_valid/'\n",
    "h_images_dense = 'Train_TN_data/horizontal_annotated_images_as_np_arrays_valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ed7fb-446c-4222-9b68-7446f25c17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shutil.copytree(va_images_dense, 'Train_TN_data/valid_all_image_density_files/', dirs_exist_ok=True)\n",
    "shutil.copytree(vn_images_dense, 'Train_TN_data/valid_all_image_density_files/', dirs_exist_ok=True)\n",
    "shutil.copytree(h_images_dense, 'Train_TN_data/valid_all_image_density_files/', dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee5fc9-86d2-4d1c-8580-a057f1b19ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do this for a sample and then in the loop\n",
    "valid_files_path = \"Train_TN_data/valid_all_image_density_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d587866-4571-473c-8c5d-db768f4a439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_im_and_map_contents = os.listdir(valid_files_path)\n",
    "\n",
    "# sort these - ALWAYS sort these as the order is always messed up on HCC\n",
    "valid_im_and_map_contents.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0599f-0a66-4ba5-b4a9-e01ef04bf7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_im_and_map_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9806a0-dffe-4a47-8ed9-5f579a116614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the names of the image (npy files)\n",
    "valid_im_names = [item for item in valid_im_and_map_contents if item.split(\".\")[0][-3:] != 'map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b6719-f944-4ed1-9810-bc92e6ba3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_im_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7cea3d-0425-45da-aaff-52d5f1a64926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_im_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e6ff6-df8e-4928-83cd-514465c71450",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create the subwindows for all train data\n",
    "catch_all_image_subwindows_valid = []\n",
    "catch_all_dense_subwindows_valid = []\n",
    "catch_valid_names = []\n",
    "for image in valid_im_names:\n",
    "    valid_ims, valid_maps, valid_names = create_subwindows_and_counts(image, valid_files_path, stride = 8, kernel_size = 32)\n",
    "    catch_all_image_subwindows_valid.append(valid_ims)\n",
    "    catch_all_dense_subwindows_valid.append(valid_maps)\n",
    "    catch_valid_names.append(valid_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f3626-6a57-4595-8ff4-8b6041b3aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check names\n",
    "np.mean(valid_im_names == catch_valid_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd9e47-a100-4743-b8d8-6c2bc64a79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the images\n",
    "valid_im_stack = np.vstack(catch_all_image_subwindows_valid)\n",
    "print(valid_im_stack.shape)\n",
    "\n",
    "# stack the subcounts\n",
    "valid_count_stack = np.hstack(catch_all_dense_subwindows_valid)\n",
    "print(valid_count_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e88dc3-0975-40df-97d1-222082f2f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "393216/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08254f5a-bd55-4bce-be89-78bd83034206",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 31\n",
    "for i in range(index):\n",
    "    print(np.mean(valid_im_stack[12288*index:12288+12288*index,:,:,:] == catch_all_image_subwindows_valid[index]), np.mean(valid_count_stack[12288*index:12288+12288*index,] == catch_all_dense_subwindows_valid[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269c15c-aaad-4de1-9125-78978f785b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a little more sanity checks to make sure the stacking is correctly done\n",
    "# for images\n",
    "\n",
    "# need to save these files\n",
    "valid_save_path = 'Train_TN_data/final_valid_sub_windows_and_counts'\n",
    "\n",
    "# save the sub images\n",
    "np.save(valid_save_path + \"/\" + \"valid_sub_windows.npy\", valid_im_stack)\n",
    "# save the sub counts\n",
    "np.save(valid_save_path + \"/\" + \"valid_sub_counts.npy\", valid_count_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c607ad-29bb-4323-b397-1289e9f83d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subwindows_and_counts(image, numpy_folder, stride = 8, kernel_size = 32):\n",
    "    im_name = image.split(\".\")[0]\n",
    "    im_file = im_name + '.npy'\n",
    "    count_file = im_name + '_density_map.npy'\n",
    "    # load the image and the count numpy files\n",
    "    loaded_im_file = np.load(numpy_folder + '/' + im_file)\n",
    "    loaded_count_file = np.load(numpy_folder + '/' + count_file)\n",
    "        \n",
    "    # create the subwindows and counts as follows\n",
    "    img_height = loaded_im_file.shape[0]\n",
    "    img_width = loaded_im_file.shape[1]\n",
    "    \n",
    "    density_sums = []\n",
    "    catch_image = []\n",
    "    for i in  range(0, img_height, stride):\n",
    "        for j in range(0, img_width, stride):\n",
    "            sub_window = loaded_im_file[i: i + kernel_size, j : j + kernel_size,:]\n",
    "            density = loaded_count_file[i: i + kernel_size, j : j + kernel_size]\n",
    "            dense_sum = np.sum(density)\n",
    "            density_sums.append(dense_sum)\n",
    "            sub_window = resize(sub_window, (32, 32,3))\n",
    "            catch_image.append(sub_window)\n",
    "\n",
    "    # save the combined subwindows and counts\n",
    "    return(catch_image,density_sums, im_file)\n",
    "\n",
    "# let's do this for a sample and then in the loop\n",
    "train_files_path = \"Train_TN_data/train_all_image_density_files/\"\n",
    "\n",
    "train_im_and_map_contents = os.listdir(train_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e19c5-ecdd-4543-9734-5c8e4cfe860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_im_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c81d5-930e-4695-89ef-71eb7ec09fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort these - ALWAYS sort these as the order is always messed up on HCC\n",
    "train_im_and_map_contents.sort()\n",
    "print(len(train_im_and_map_contents))\n",
    "# get only the names of the image (npy files)\n",
    "train_im_names = [item for item in train_im_and_map_contents if item.split(\".\")[0][-3:] != 'map']\n",
    "train_im_names = [item for item in train_im_names if item != '.ipynb_checkpoints']\n",
    "print(len(train_im_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a75a3-bab1-4853-b7d1-697f48de7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_im_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe7c38-3f19-47f5-8a80-8b9483117cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the subwindows for all train data\n",
    "catch_all_image_subwindows_train = []\n",
    "catch_all_dense_subwindows_train = []\n",
    "catch_train_names = []\n",
    "for image in train_im_names:\n",
    "    train_ims, train_maps, train_names = create_subwindows_and_counts(image, train_files_path, stride = 8, kernel_size = 32)\n",
    "    catch_all_image_subwindows_train.append(train_ims)\n",
    "    catch_all_dense_subwindows_train.append(train_maps)\n",
    "    catch_train_names.append(train_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfp_for_TN)",
   "language": "python",
   "name": "tfp_for_tn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
