{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acc4919-b78d-48df-b991-71429ed1cccd",
   "metadata": {},
   "source": [
    "##### Goal is to write a code that can do a proper grid search, and chose the hyper-parameters that best get the work done - Focus on reducing the mae, rmse, widths and increasing the coverages. Let's not plot the plots at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7265f30c-277c-49a6-9aff-13b13ad860db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 10:49:02.370533: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-22 10:49:02.834428: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e5ccd9-dcb1-405f-98ec-978bcff5dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfp particulars\n",
    "tfd = tfp.distributions\n",
    "root = tfd.JointDistributionCoroutine.Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5632ae56-f148-4600-b96a-7dc9ddce0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test data\n",
    "\n",
    "def split_train_test_data(sub_image_df, path_to_df, n_forecasting):\n",
    "    # join the paths\n",
    "    complete_path_to_df = os.path.join(path_to_df, sub_image_df)\n",
    "    # read the csv \n",
    "    read_df = pd.read_csv(complete_path_to_df)\n",
    "\n",
    "    # split the data into train and test\n",
    "    train_df = read_df.iloc[:-n_forecasting, :]\n",
    "    print(train_df.shape)\n",
    "    test_df = read_df.iloc[-n_forecasting:,:]\n",
    "    print(test_df.shape)\n",
    "\n",
    "    # get the obs data\n",
    "    train_y = train_df['tassel_count']\n",
    "    test_y = test_df['tassel_count']\n",
    "\n",
    "    # make these float 32 for bayes ts implementation\n",
    "    train_y = train_y.astype(\"float32\")\n",
    "    test_y = test_y.astype(\"float32\")\n",
    "\n",
    "    # these needs to be returned\n",
    "\n",
    "    # also split the covariate data\n",
    "    # but add an intercept before the split?\n",
    "    read_df.insert(0, 'intercept', np.repeat(1, read_df.shape[0]))\n",
    "    # make this float32 for bayes ts implementation\n",
    "    read_df['intercept'] = read_df['intercept'].astype(\"float32\")\n",
    "\n",
    "    # now can extract the covariate data\n",
    "    X_preds = read_df.drop(['tassel_count'], axis = 1).astype(\"float32\")\n",
    "    X_preds = X_preds.values\n",
    "    print(X_preds.shape)\n",
    "    n_preds = X_preds.shape[-1]\n",
    "    return(train_y, test_y, X_preds, n_preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de09a3d2-033d-49e2-9398-d4d5d01c1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the plot function\n",
    "def plot_tassel_count_data(train_data, test_data, df_no, fig, ax):\n",
    "    # if not fig_ax:\n",
    "    #     fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    # else:\n",
    "    #     fig, ax = fig_ax\n",
    "    ax.plot(train_data, color = 'blue', label=\"training data\")\n",
    "    ax.plot(test_data, color = 'lightcoral', label=\"testing data\")\n",
    "    ax.legend()\n",
    "    ax.set(\n",
    "        ylabel=\"Tassel counts\" ,\n",
    "        xlabel=\"Time\",\n",
    "        title = \"Tassel count distribution for sub image \" + str(df_no)\n",
    "    )\n",
    "    fig.autofmt_xdate()\n",
    "    fig.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfadd1de-c366-47d8-a941-7ec6c0db1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prioirs_and_x_beta(X_pred, n_pred, beta_mu, beta_sigma, noise_scale):\n",
    "    beta = yield root(tfd.Sample(\n",
    "        tfd.Normal(beta_mu, beta_sigma),\n",
    "        sample_shape=n_pred,\n",
    "        name='beta'))\n",
    "    x_beta = tf.einsum('ij,...j->...i', X_pred, beta)\n",
    "\n",
    "    noise_sigma = yield root(tfd.HalfNormal(scale=noise_scale, name='noise_sigma'))\n",
    "\n",
    "    intercept_data = X_pred[:,0]\n",
    "\n",
    "    return (x_beta, intercept_data, noise_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8eda5d-97f3-4548-926d-0d6b6f8836ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tp = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4762610-07cb-403e-9f29-6d4b107d8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_ar_latent(preds_data, n_pred, ar_scale, rho_lower, beta_mu, beta_sigma, noise_scale, training=True):\n",
    "\n",
    "    @tfd.JointDistributionCoroutine\n",
    "    def model_with_latent_ar():\n",
    "        x_beta, intercept_data, noise_sigma = yield from get_prioirs_and_x_beta(preds_data, n_pred, beta_mu, beta_sigma, noise_scale)\n",
    "        \n",
    "        # Latent AR(1)\n",
    "        ar_sigma = yield root(tfd.HalfNormal(ar_scale, name='ar_sigma'))\n",
    "        rho = yield root(tfd.Uniform(rho_lower, 1., name='rho'))\n",
    "        def ar_fun(y):\n",
    "            loc = tf.concat([tf.zeros_like(y[..., :1]), y[..., :-1]],\n",
    "                            axis=-1) * rho[..., None]\n",
    "            return tfd.Independent(\n",
    "                tfd.Normal(loc=loc, scale=ar_sigma[..., None]),\n",
    "                reinterpreted_batch_ndims=1)\n",
    "        temporal_error = yield tfd.Autoregressive(\n",
    "            distribution_fn=ar_fun,\n",
    "            sample0=tf.zeros_like(intercept_data),\n",
    "            num_steps=intercept_data.shape[-1],\n",
    "            name='temporal_error')\n",
    "\n",
    "        # Linear prediction\n",
    "        y_hat = x_beta + temporal_error\n",
    "        if training:\n",
    "            y_hat = y_hat[..., :train_tp]\n",
    "\n",
    "        # Likelihood\n",
    "        observed = yield tfd.Independent(\n",
    "            tfd.Normal(y_hat, noise_sigma[..., None]),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "            name='observed'\n",
    "        )\n",
    "\n",
    "    return model_with_latent_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a5dab2-55d8-4349-9a62-47c912b335b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run the following for each bayesian ts model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86bd050-f02d-4058-b02d-6f634bccac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to plot the X_beta and temporal errors\n",
    "def preds_and_temoral_error(mcmc_samples_data, preds_data, n_total_time_points, nchains):\n",
    "    # plot components\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 7.5), sharex=True)\n",
    "\n",
    "    beta = mcmc_samples_data[0]\n",
    "    seasonality_posterior = tf.einsum('ij,...j->...i', preds_data, beta)\n",
    "    temporal_error = mcmc_samples_data[-1]\n",
    "\n",
    "    for i in range(nchains):\n",
    "        ax[0].plot(np.arange(n_total_time_points), seasonality_posterior[-100:, i, :].numpy().T, alpha=.05);\n",
    "        ax[1].plot(np.arange(n_total_time_points), temporal_error[-100:, i, :].numpy().T, alpha=.05);\n",
    "\n",
    "    ax[0].set_title('X_beta effect')\n",
    "    ax[1].set_title('Temporal error')\n",
    "    ax[1].set_xlabel(\"Day\")\n",
    "    fig.autofmt_xdate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129934b-47cd-4c0a-80b2-d73dfa44b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the forecated and actual values\n",
    "def forecasted_and_actual_values_plot(ppc_sample_data, train_counts, test_counts, df_no, fig, ax):\n",
    "    fitted_with_forecast = ppc_sample_data[-1].numpy()\n",
    "    \n",
    "    ax.plot(np.arange(20), fitted_with_forecast[:250, 0, :].T, color='gray', alpha=.1);\n",
    "    ax.plot(np.arange(20), fitted_with_forecast[:250, 1, :].T, color='gray', alpha=.1);\n",
    "    \n",
    "    plot_tassel_count_data(train_counts, test_counts, df_no, fig, ax)\n",
    "    average_forecast = np.mean(fitted_with_forecast, axis=(0, 1)).T\n",
    "    ax.plot(np.arange(20), average_forecast, ls='--', label='latent AR forecast', color = 'red', alpha=.5);\n",
    "    plt.xticks(np.arange(20))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a9777-8d1c-47f9-a611-199187670792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get the posteriors and the trace plots with az\n",
    "def get_nuts_values_and_posterior_plots(mcmc_samples_bts, sampler_stats_bts):\n",
    "    nuts_trace_ar_latent = az.from_dict(posterior={k:np.swapaxes(v.numpy(), 1, 0) for k, v in mcmc_samples_bts._asdict().items()},\n",
    "    sample_stats = {k:np.swapaxes(sampler_stats_bts[k], 1, 0)for k in [\"target_log_prob\", \"diverging\", \"accept_ratio\", \"n_steps\"]})\n",
    "\n",
    "    axes = az.plot_trace(nuts_trace_ar_latent, var_names=['beta', 'ar_sigma', 'rho', 'noise_sigma'], compact=True);\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return(nuts_trace_ar_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07ee75-3f58-47d2-bb45-5bab5921bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_vals(nuts_model):\n",
    "    rho_values = nuts_model.posterior.rho\n",
    "    print(rho_values.shape)\n",
    "    ar_sigma_values = nuts_model.posterior.ar_sigma\n",
    "    print(ar_sigma_values.shape)\n",
    "    noise_sigma_values = nuts_model.posterior.noise_sigma\n",
    "    print(noise_sigma_values.shape)\n",
    "    beta_vals_all = nuts_model.posterior.beta\n",
    "    print(beta_vals_all.shape)\n",
    "\n",
    "    return(rho_values, ar_sigma_values, noise_sigma_values, beta_vals_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db4213-3e40-47e8-b92f-196758adb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for traceplots\n",
    "color_list = ['cornflowerblue', 'lightsteelblue', 'blue', 'mediumblue', 'cyan', 'deepskyblue', 'steelblue', 'dodgerblue', 'lightslategray', 'mediumslateblue',\n",
    "             'lightblue', 'teal', 'royalblue', 'indianred', 'deepskyblue', 'honeydew', 'lightseagreen', 'turquoise', 'cadetblue', 'tan', 'moccasin', 'burlywood',\n",
    "             'peachpuff', 'powderblue', 'mediumaquamarine', 'powderblue', 'thistle', 'lavender', 'lightcyan', 'darkseagreen', 'honeydew', 'lightsteelblue', 'cadetblue']\n",
    "len(color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59010f3-88b6-45de-ad81-43f8d0f0572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color palattes for freq polygons\n",
    "color_palletes_betas = ['Greys','Purples','Blues','Greens','Oranges','Reds','YlOrBr','YlOrRd','OrRd','PuRd','RdPu','BuPu','GnBu','PuBu','YlGnBu',\n",
    " 'PuBuGn','BuGn','YlGn','Greys','Purples','Blues','Greens','Oranges','Reds','YlOrBr','YlOrRd','OrRd','PuRd','RdPu','BuPu','GnBu','PuBu', 'YlGnBu']\n",
    "len(color_palletes_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05364483-7d17-4669-87a5-28c3fe248ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for this?\n",
    "\n",
    "# for a chosen parameter - param\n",
    "\n",
    "def get_trace_plots(ax, param, color, string_params):\n",
    "    ax.plot(param.T, color = color, alpha = 0.5)\n",
    "    ax.set_title(\"Trace plot for \" + string_params, fontsize=10, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7ffed-c6ea-45bf-bea4-cc3f80e6dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for this?\n",
    "def get_freq_curves(ax, param, color_palette, string_params):\n",
    "    sns.kdeplot(data=param.T, fill=False, ax=ax, legend = False, palette = color_palette)\n",
    "    ax.set_title(\"Frequency plot for \" + string_params, fontsize=10, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da91c9-3f44-44be-b9a7-6070d5dcefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_Bayes_TS_model(csv_file, forecasting_steps, path_to_preprocessed_dfs, sub_image_number, n_features, nchains, ar_scale, rho_lower, beta_mu, beta_sigma, noise_scale):\n",
    "    # get the counts and predictors\n",
    "    Train_Y, Test_Y, X_preds_only, n_predictors = split_train_test_data(csv_file, path_to_preprocessed_dfs, forecasting_steps)\n",
    "    # plot the train and test counts\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    # plot_tassel_count_data(Train_Y, Test_Y, sub_image_number, fig, ax)\n",
    "    # define the mcmc function\n",
    "    run_mcmc = tf.function(tfp.experimental.mcmc.windowed_adaptive_nuts, autograph=False, jit_compile=True)\n",
    "    # define the latet ar model\n",
    "    gam_with_latent_ar = generate_model_ar_latent(X_preds_only, n_predictors, ar_scale, rho_lower, beta_mu, beta_sigma, noise_scale, training=True)\n",
    "    # plot samples from prior predictive distribution\n",
    "    # plt.figure(figsize = (10,8))\n",
    "    # plt.plot(tf.transpose(gam_with_latent_ar.sample(500)[-1]))\n",
    "    # plt.show()\n",
    "    # run the mcmc with nuts sampler\n",
    "    mcmc_samples, sampler_stats = run_mcmc(1000, gam_with_latent_ar, n_chains=4, num_adaptation_steps=1000, seed=tf.constant([36245, 734565], dtype=tf.int32), observed=Train_Y.T)\n",
    "    # get the posterior values for parameters and the posterioir predictions\n",
    "    gam_with_latent_ar_full = generate_model_ar_latent(X_preds_only, n_predictors, ar_scale, rho_lower, beta_mu, beta_sigma, noise_scale, training=False)\n",
    "    posterior_dists, ppc_samples = gam_with_latent_ar_full.sample_distributions(value=mcmc_samples)\n",
    "    # plot the posteriors ? for betas and temporal errors\n",
    "    # preds_and_temoral_error(mcmc_samples, X_preds_only, n_total_time_points = 20, nchains = 4)\n",
    "    # Plot the forecated and actual values\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    # fig_ax = (fig, ax)\n",
    "    # forecasted_and_actual_values_plot(ppc_samples, Train_Y, Test_Y, sub_image_number, fig, ax)\n",
    "    # Get the posterior plots\n",
    "    nuts_output = get_nuts_values_and_posterior_plots(mcmc_samples, sampler_stats)\n",
    "    # recreate this plot\n",
    "    rho_values, ar_sigma_values, noise_sigma_values, beta_vals_all = posterior_vals(nuts_output)\n",
    "    # get the betas gathered and plot them all in a single plot\n",
    "    all_betas = [beta_vals_all[: ,: , i] for i in range(n_features)]\n",
    "    # # recreate the plot\n",
    "    # fig, axs = plt.subplots(4, 2, figsize=(10, 10))\n",
    "    # get_trace_plots(axs[0, 0], rho_values, 'palevioletred', 'rho')\n",
    "    # get_trace_plots(axs[1, 0], noise_sigma_values, 'blue', 'noise sigma')\n",
    "    # get_trace_plots(axs[2, 0], ar_sigma_values, 'indianred', 'ar sigma')\n",
    "    # for i in range(33):\n",
    "    #     get_trace_plots(axs[3, 0], all_betas[i], color_list[i], 'betas')\n",
    "    # fig.tight_layout()\n",
    "    # get_freq_curves(axs[0, 1], rho_values, 'Blues', 'rho')\n",
    "    # get_freq_curves(axs[1, 1], noise_sigma_values, 'Purples', 'noise sigma')\n",
    "    # get_freq_curves(axs[2, 1], ar_sigma_values, 'Greens', 'ar sigma')\n",
    "    # for i in range(33):\n",
    "    #     get_freq_curves(axs[3, 1], all_betas[i], color_palletes_betas[i], 'betas')\n",
    "    # fig.tight_layout()\n",
    "    # # plt.plot(all_betas[i].T, color = color_list[i], alpha = 0.5)\n",
    "    # # figure_path_and_name = figure_folder_path + '/' + 'all_trace_plots_sub_' + str(sub_image_number) + '.png'\n",
    "    # # plt.savefig(figure_path_and_name)\n",
    "    # plt.show()\n",
    "    # get the forecasted values\n",
    "    forecasted_values = ppc_samples[-1].numpy()\n",
    "    averaged_forecast = np.mean(forecasted_values, axis=(0, 1)).T\n",
    "    print(forecasted_values.shape, averaged_forecast.shape)\n",
    "    # we may need to store the averaged forecasts - extract these first for the test set only\n",
    "    test_averaged_forecast = averaged_forecast[-forecasting_steps:]\n",
    "    test_all_forecasts = forecasted_values[:,:,-forecasting_steps:]\n",
    "    # create a dataframe for true and averaged forecasts for test data\n",
    "    final_forecasted_values = pd.DataFrame(zip(Test_Y, test_averaged_forecast), columns = ['True_value', 'Forecasted_value'])\n",
    "    # save the avrage and all forecasts for future use\n",
    "    # avg_frcst_file_name = forecasts_folder_path + '/' + 'averaged_forecasts_sub_' + str(sub_image_number) + '.csv'\n",
    "    # final_forecasted_values.to_csv(avg_frcst_file_name, index = False)\n",
    "    # all_frcst_file_name = forecasts_folder_path + '/' + 'all_forecasts_sub_' + str(sub_image_number) + '.npy'\n",
    "    # np.save(all_frcst_file_name, test_all_forecasts)\n",
    "    # get the parameter summary\n",
    "    parameter_summary = az.summary(nuts_output)\n",
    "    # parameter_summary_file_name = forecasts_folder_path + '/' + 'posterior_parameter_summary_sub_' + str(sub_image_number) + '.csv'\n",
    "    # parameter_summary.to_csv(parameter_summary_file_name, index = False)\n",
    "    # save the posterior predicted values for other parameters\n",
    "    # rho_file_name = forecasts_folder_path + '/' + 'rho_values_sub_' + str(sub_image_number) + '.npy'\n",
    "    # np.save(rho_file_name, rho_values)\n",
    "    # noise_sigma_file_name = forecasts_folder_path + '/' + 'noise_sigma_values_sub_' + str(sub_image_number) + '.npy'\n",
    "    # np.save(noise_sigma_file_name, noise_sigma_values)\n",
    "    # ar_sigma_file_name = forecasts_folder_path + '/' + 'ar_sigma_values_sub_' + str(sub_image_number) + '.npy'\n",
    "    # np.save(ar_sigma_file_name, ar_sigma_values)\n",
    "    # beta_file_name = forecasts_folder_path + '/' + 'beta_values_sub_' + str(sub_image_number) + '.npy'\n",
    "    # np.save(beta_file_name, all_betas)\n",
    "    return(final_forecasted_values, test_all_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca57ce-5053-4527-a4aa-b874a77030d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all these in a function? - note this function has 12 in the for loop for the 12 sub images - this need to change when we have overlapping data\n",
    "def end_to_end_metrics(sub_image_files_list, sub_image_numbers, forecasting_steps, path_to_preprocessed_dfs,  n_features, nchains, ar_scale, rho_lower, beta_mu, beta_sigma, noise_scale):\n",
    "    # get the average and all forecasts\n",
    "    all_dfs = []\n",
    "    for i in np.arange(12):\n",
    "        get_df = fit_Bayes_TS_model(sub_image_files_list[i], forecasting_steps, path_to_preprocessed_dfs, sub_image_numbers[i], n_features, nchains, ar_scale, rho_lower, beta_mu, beta_sigma, noise_scale)\n",
    "        all_dfs.append(get_df)\n",
    "\n",
    "    # collect only the average forecasts dfs\n",
    "    df_only = []\n",
    "    for i in range(len(all_dfs)):\n",
    "        df = all_dfs[i][0]\n",
    "        df_only.append(df)\n",
    "\n",
    "    # combine all the dfs - to get the finalized average tassel counts\n",
    "    combined_dfs = pd.concat(df_only, axis = 1)\n",
    "    # seperate the true and forecasted values\n",
    "    True_counts_df = combined_dfs[['True_value']]\n",
    "    total_true_values = True_counts_df.sum(axis = 1)\n",
    "    Forecasted_counts_df = combined_dfs[['Forecasted_value']]\n",
    "    total_forecasted_values = Forecasted_counts_df.sum(axis = 1)\n",
    "\n",
    "    # combine these two together\n",
    "    true_and_forecasted_values_df = pd.concat((total_true_values, total_forecasted_values), axis = 1)\n",
    "    true_and_forecasted_values_df.columns = [\"True_count\", \"Forecasted_count\"]\n",
    "    \n",
    "    # compute the metrics\n",
    "    rmse = np.sqrt(mean_squared_error(total_true_values, total_forecasted_values))\n",
    "    mae = mean_absolute_error(total_true_values, total_forecasted_values)\n",
    "    corr = pearsonr(total_true_values, total_forecasted_values)[0]\n",
    "    # for widths and coverages we need the following\n",
    "    # collect all the forecasted values, and sum them up to get the totals\n",
    "    all_forecasts_arrays = []\n",
    "    for i in range(len(all_dfs)):\n",
    "        all_forecasts = all_dfs[i][1]\n",
    "        all_forecasts_arrays.append(all_forecasts)\n",
    "    # sum all these up\n",
    "    output = sum(all_forecasts_arrays)\n",
    "    # reshape these\n",
    "    final_array = output.reshape(4000,7)\n",
    "    # compute percentiles\n",
    "    li_train = np.percentile(final_array, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)    \n",
    "    ui_train = np.percentile(final_array, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)\n",
    "    # compute width\n",
    "    width_train = ui_train - li_train\n",
    "    avg_width_train = width_train.mean(0)[0]\n",
    "    # compute the coverage\n",
    "    y_true = true_and_forecasted_values_df[[\"True_count\"]].values\n",
    "    ind_train = (y_true >= li_train) & (y_true <= ui_train)\n",
    "    coverage_train= ind_train.mean(0)[0]\n",
    "    # return the metrics\n",
    "    return(rmse, mae, corr, avg_width_train, coverage_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fea895-a7e2-44ea-8168-f7d3a1f4401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_steps = 7\n",
    "path_to_preprocessed_dfs = '../all_preprocessed_data/Block_0104/TS_ready_data_frames/'\n",
    "# sub_image_number = 0\n",
    "n_features = 33\n",
    "nchains = 4\n",
    "# figure_folder_path = 'figures/block_0104'\n",
    "# forecasts_folder_path = 'forecasted_counts/block_0104'\n",
    "sub_image_numbers = np.arange(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be954323-3b75-4c58-85a6-a70fb34d7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_scale_vals = [0.1, 5]\n",
    "rho_lower_vals = [-1, 0.5]\n",
    "beta_mu_vals = [0]\n",
    "beta_sigma_vals = [1, 5]\n",
    "noise_scale_vals = [1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135539f4-7d4e-415f-963f-025f6caba343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the checkpointing folder\n",
    "sub_image_files = [file for file in os.listdir(path_to_preprocessed_dfs) if file[-3:] == 'csv']\n",
    "sub_image_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645bff2-010c-4713-95d8-ee0cc13ab360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 10, 11 at the end\n",
    "im_files = ['extracted_features_sub_window_10.csv', 'extracted_features_sub_window_11.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e1150-ab1a-4676-aa1f-850521648a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_files = [i for i in sub_image_files if i not in im_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb3ad0-8784-4a97-b399-0db5f7ba4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_image_files = other_files + im_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2721494-f2a4-43d0-bc78-3b24d328adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da5ef4f-3197-4235-8653-e4ceb2adb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "catch_all_contents = []\n",
    "for ar_scale in ar_scale_vals:\n",
    "    print(ar_scale)\n",
    "    for rho_lower in rho_lower_vals:\n",
    "        print(rho_lower)\n",
    "        for beta_mu in beta_mu_vals:\n",
    "            print(beta_mu)\n",
    "            for beta_sigma in beta_sigma_vals:\n",
    "                print(beta_sigma)\n",
    "                for noise_scale in noise_scale_vals:\n",
    "                    print(noise_scale)\n",
    "                    all_metrics = end_to_end_metrics(sub_image_files, sub_image_numbers, forecasting_steps, path_to_preprocessed_dfs,\n",
    "                                                                                          n_features, nchains, ar_scale, rho_lower, beta_mu, beta_sigma, noise_scale)\n",
    "                    catch_all_contents.append(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6540521-73ad-4647-971d-f33b4304777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver the list of contents to a dataframe\n",
    "metrics_df = pd.DataFrame(catch_all_contents)\n",
    "metrics_df.columns = ['rmse', 'mae', 'corr', 'avg_width_train', 'coverage_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e68a4-72f7-455a-b737-43a2c0c7228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80738e20-2dc8-4462-b83e-b7c91d3c936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the hyper parameters to a dataframe\n",
    "all_HPO_combos = []\n",
    "for ar_scale in ar_scale_vals:\n",
    "    for rho_lower in rho_lower_vals:\n",
    "        for beta_mu in beta_mu_vals:\n",
    "            for beta_sigma in beta_sigma_vals:\n",
    "                for noise_scale in noise_scale_vals:\n",
    "                    hpo = [ar_scale, rho_lower, beta_mu, beta_sigma, noise_scale]\n",
    "                    all_HPO_combos.append(hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c965af5-b89a-4038-bc24-e23b9e4b5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_df = pd.DataFrame(all_HPO_combos)\n",
    "hpo_df.columns = ['ar_scale', 'rho_lower', 'beta_mu', 'beta_sigma', 'noise_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cf0ef-58c6-4133-b8fc-a44a6397b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df82aab6-7b5a-4221-9493-ea1116236e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataframes\n",
    "combined_df = pd.concat((hpo_df, metrics_df ), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c7ece-209e-44ec-ab71-4b7ad8921c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7e65c-867b-492d-b237-f6eb7d83aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store this df for later comparisons\n",
    "combined_df.to_csv(\"Store_metric_results/hpo_metrics_for_block0104.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfp_env_TN_CPU)",
   "language": "python",
   "name": "tfp_env_tn_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
